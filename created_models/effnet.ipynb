{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9097401,"sourceType":"datasetVersion","datasetId":5490352}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"PXL_classification","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# # THEN FEEL FREE TO DELETE THIS CELL.\n# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# # NOTEBOOK.\n\n# import os\n# import sys\n# from tempfile import NamedTemporaryFile\n# from urllib.request import urlopen\n# from urllib.parse import unquote, urlparse\n# from urllib.error import HTTPError\n# from zipfile import ZipFile\n# import tarfile\n# import shutil\n\n# CHUNK_SIZE = 40960\n# DATA_SOURCE_MAPPING = 'seen-dataset-12-class:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5490352%2F9097401%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T042049Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3aace99ddd145156558193a8f31ce17bbf13c2e2758480d5a99937c3592b6eef74870bf4b2e01927cc13420701eb73627166f3ba9443a9834ef728568661aeca5c7a648deb81cff9a33ea9d23baf1ce968ec04972e11173139a8cf3a74c0e61c16cbac1971291e8d5940460a2d6126ff1ea3e3d2dab15098b704be59d4ac639d12eb0f327c5eb1108e2f75ca30f83292d2f33412b9d62a6bb99a0ca95ca411c57cc84ef3fb77f77ac2b2b7797f4d74bd0a12a07be140e3681a92cbaedfe0b8ce2253aafb3291a16461e649154e3c583e9dfe2563138dd72fe5afcbc451ae5c21d834798ce68c42db01afb094a7c19b64ee0540ca66010864b070121d83852478'\n\n# KAGGLE_INPUT_PATH='/kaggle/input'\n# KAGGLE_WORKING_PATH='/kaggle/working'\n# KAGGLE_SYMLINK='kaggle'\n\n# !umount /kaggle/input/ 2> /dev/null\n# shutil.rmtree('/kaggle/input', ignore_errors=True)\n# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\n# try:\n#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n# except FileExistsError:\n#   pass\n# try:\n#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n# except FileExistsError:\n#   pass\n\n# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n#     directory, download_url_encoded = data_source_mapping.split(':')\n#     download_url = unquote(download_url_encoded)\n#     filename = urlparse(download_url).path\n#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n#     try:\n#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n#             total_length = fileres.headers['content-length']\n#             print(f'Downloading {directory}, {total_length} bytes compressed')\n#             dl = 0\n#             data = fileres.read(CHUNK_SIZE)\n#             while len(data) > 0:\n#                 dl += len(data)\n#                 tfile.write(data)\n#                 done = int(50 * dl / int(total_length))\n#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n#                 sys.stdout.flush()\n#                 data = fileres.read(CHUNK_SIZE)\n#             if filename.endswith('.zip'):\n#               with ZipFile(tfile) as zfile:\n#                 zfile.extractall(destination_path)\n#             else:\n#               with tarfile.open(tfile.name) as tarfile:\n#                 tarfile.extractall(destination_path)\n#             print(f'\\nDownloaded and uncompressed: {directory}')\n#     except HTTPError as e:\n#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n#         continue\n#     except OSError as e:\n#         print(f'Failed to load {download_url} to path {destination_path}')\n#         continue\n\n# print('Data source import complete.')\n","metadata":{"id":"18rySelSN-MH","execution":{"iopub.status.busy":"2024-08-08T06:52:25.443469Z","iopub.execute_input":"2024-08-08T06:52:25.443733Z","iopub.status.idle":"2024-08-08T06:52:25.452816Z","shell.execute_reply.started":"2024-08-08T06:52:25.443706Z","shell.execute_reply":"2024-08-08T06:52:25.452230Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## EfficientNet Architecture","metadata":{"id":"EPKq_BCUN-MK"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport torchvision","metadata":{"id":"tq-jFYP8N-ML","execution":{"iopub.status.busy":"2024-08-08T06:52:25.457065Z","iopub.execute_input":"2024-08-08T06:52:25.457291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1,act=True, bias=False):\n\n        super().__init__()\n        # same padding quick mapping:\n        # k=1 -> p =0, k=3 -> p=1, k=5 -> p=2\n        padding = kernel_size // 2\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = padding, bias=bias, groups=groups)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.activation = nn.SiLU() if act else nn.Identity()\n\n    def forward(self, x):\n        x= self.conv(x)\n        x= self.batch_norm(x)\n        return self.activation(x)","metadata":{"id":"ZFiXkWrZN-MM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SqueezeAndExcitationBlock(nn.Module):\n    def __init__(self, in_channels,reduction_ratio=16): #reduction_ratio to reduce computation, a hyperparameter. take r=16 for balance in complexity and capacity as in SeNet paper\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n        self.fc1 = nn.Linear(in_channels , in_channels//reduction_ratio, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(in_channels//reduction_ratio, in_channels, bias =False)\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self,x):\n\n        x_out = self.squeeze(x)\n        x_out = torch.flatten(x_out,1)\n        x_out = self.relu(self.fc1(x_out))\n        x_out = self.sigmoid(self.fc2(x_out))\n\n        x_out = x_out[:,:,None,None]\n\n        scaled = x * x_out\n        return scaled\n","metadata":{"id":"HvBkkVkNN-MM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class SqueezeAndExcitationBlockUncheckedLikelyWrong(nn.Module):\n#     def __init__(self, in_channels, reduction_ratio=16):\n#         super().__init__()\n#         self.squeeze = nn.AdaptiveAvgPool2d(1)\n#         self.excitation = nn.Sequential(\n#             nn.Linear(in_channels, in_channels // reduction_ratio, bias=False),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(in_channels // reduction_ratio, in_channels, bias=False),\n#             nn.Sigmoid()\n#         )\n\n#     def forward(self, x):\n#         batch, channels, _, _ = x.size()\n#         y = self.squeeze(x).view(batch, channels)\n#         y = self.excitation(y).view(batch, channels, 1, 1)\n#         return x * y\n","metadata":{"id":"uqopYI_KN-MM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MobileConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion_ratio, reduction_ratio):\n        super().__init__()\n\n        exp_out_channels = in_channels * expansion_ratio\n\n        #residual connection only when it is not being downsampled in any way\n        self.add_res = in_channels == out_channels and stride ==1\n        self.conv1 = CNNBlock(in_channels, exp_out_channels, 1,1) if expansion_ratio > 1 else nn.Identity()\n\n        #depthwise convolution\n        self.conv2 = CNNBlock(exp_out_channels, exp_out_channels, kernel_size, stride, exp_out_channels)\n        self.se = SqueezeAndExcitationBlock(exp_out_channels,reduction_ratio)\n        self.conv3 = CNNBlock(exp_out_channels,out_channels, 1,1, act=False) #hatched line features means no activation\n\n        self.sd = StochasticDepth(0.75)\n\n    def forward(self, x):\n        x_out = self.conv3(self.se(self.conv2(self.conv1(x))))\n\n        if self.add_res:\n            x_out = x + x_out\n\n        x_out = self.sd(x_out)\n\n        return x_out","metadata":{"id":"4GdK7iBbN-MN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationBlock(nn.Module):\n    def __init__(self,in_channels, n_classes, dropout_prob):\n        super().__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc = nn.Linear(in_channels, n_classes)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self,x):\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x= torch.flatten(x,1)\n        x = self.fc(x)\n        return self.sigmoid(x)\n","metadata":{"id":"PPSbQMyBN-MN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StochasticDepth(nn.Module):\n    def __init__(self, p=0.75):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n\n        rand_mask = torch.rand((x.shape[0], 1,1,1),  dtype=x.dtype, device=x.device)\n        binary_mask = torch.floor(rand_mask) #TODO\n\n        if self.training:   x = x/self.p * binary_mask\n\n        return x","metadata":{"id":"10pCqBarN-MO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EffNet(nn.Module):\n    def __init__(self, model_name, in_channels=3, n_classes=25, show_output_dims=False):\n        super().__init__()\n        self.show = show_output_dims\n        self.model_name = model_name\n        self.config = Config()\n        self.stages = self.config.stages\n        self.phis = self.config.phis[model_name]\n\n        #parameters\n        phi, res, dropout_p = self.phis\n        self.calc_coeffs(phi)\n\n        #define network\n        self.network = nn.ModuleList([])\n        self.channels =[]\n\n        #baseline stage 1\n        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[0]\n        self.add_layers(3, operator, channels, layers, kernel_size, stride) #rgb input layer, TODO: check for errors\n        print(operator)\n\n        #remaining stages: 9 stages ko 7 stages (2-8) lai\n        for i in range(1, len(self.stages)-1):\n\n            if i==1:\n                reduction_ratio=4\n            else:\n                reduction_ratio=24\n\n            operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[i]\n            self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride, expansion_ratio, reduction_ratio)\n            print(operator)\n\n\n        #final stage: conv1x1 and classifier\n        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[-1]\n        self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride) #the conv layer\n        print(operator)\n        self.network.append(ClassificationBlock(self.channels[-1], n_classes,dropout_p)) #the classifier block\n\n\n\n    def forward(self, x):\n\n        for stage_num,module in enumerate(self.network):\n\n            x= module(x)\n\n            shape = x.shape\n            if self.show: print(f\"shape of stage{stage_num} : {shape}\")\n\n        return x\n\n\n    def add_layers(self, in_channels, operator, channels, layers, kernel_size, stride, *args):\n\n        channels, layers = self.update_dw(channels, layers)\n\n        if layers == 1:\n            self.network.append(operator(in_channels, channels, kernel_size, stride, *args))\n        else:\n            #the first\n            self.network.append(operator(in_channels, channels, kernel_size, 1, *args))\n\n            #the remaining except first and last: works if there are >3 layers\n            for _ in range(layers-2):\n                self.network.append(operator(channels, channels,kernel_size, 1, *args))\n\n            #final layer with stride dependent on the stage\n            self.network.append(operator(channels,channels, kernel_size, stride, *args))\n\n        self.channels.append(channels)\n\n    # for models higher than the basseline:\n\n    def calc_coeffs(self, phi, alpha=1.2, beta =1.1): #alpha and beta from EffNet paper, calculated through grid search. We dont use gamma but use the resolution from config\n        # in every higher model, the channels is multiplied by width (beta^phi) layers is multiplied by depth (alpha^phi)\n        self.depth = alpha ** phi\n        self.width = beta ** phi\n\n    def update_dw(self,channels, layers):\n        return int(channels * self.width), int(layers * self.depth)\n\n","metadata":{"id":"1YBFEHy8N-MO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    stages = [\n            # [Operator(F), Channels, Layers, Kernel, Stride, Expansion Ratio]\n            [CNNBlock, 32, 1, 3, 2, 1],\n            [MobileConvBlock, 16, 1, 3, 1, 1],\n            [MobileConvBlock, 24, 2, 3, 2, 6],\n            [MobileConvBlock, 40, 2, 5, 2, 6],\n            [MobileConvBlock, 80, 3, 3, 2, 6],\n            [MobileConvBlock, 112, 3, 5, 1, 6],\n            [MobileConvBlock, 192, 4, 5, 2, 6],\n            [MobileConvBlock, 320, 1, 3, 1, 6],\n            [CNNBlock, 1280, 1, 1, 1, 0]\n    ]\n\n    phis = {\n            # BX : (phi, resolution, dropout)\n            \"B0\" : (0, 224, 0.2),\n            \"B1\" : (0.5, 240, 0.2),\n            \"B2\" : (1, 260, 0.3),\n            \"B3\" : (2, 300, 0.3),\n            \"B4\" : (3, 380, 0.4),\n            \"B5\" : (4, 456, 0.4),\n            \"B6\" : (5, 528, 0.5),\n            \"B7\" : (6, 600, 0.5)\n    }","metadata":{"id":"rhvHxjI2N-MP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name =\"B0\"\nmodel = EffNet(model_name = model_name, in_channels = 3, n_classes = 25, show_output_dims=False).to(device)\nres = Config().phis[model_name][1]\nprint(res)\n\nx= torch.randn((5, 3, res, res)).to(device)\nout= model(x)\nprint(out.shape)","metadata":{"id":"NwEF2cAeN-MP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"id":"QT-5MWj3N-MP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"id":"Xaa63l6cN-MQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ =summary(model,x)","metadata":{"id":"ULc-a_s-N-MR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Training, Validating and Saving the model","metadata":{"id":"G8FpwjNNN-MR"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms, ToTensor\nfrom tqdm import tqdm","metadata":{"id":"OwwpBcC1N-MR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/train\"\nval_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/val\"\ntest_dir =\"\"","metadata":{"id":"dADP73m9N-MR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data loaders for mean calculation\ntransforms_func = transforms.Compose([transforms.Resize((224,224)),\n                                      ToTensor()])\ntrain_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\nbatch_size = 32\ntrain_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=False)\n","metadata":{"id":"f0x6J6h0N-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ds_mean_std(dataloader):\n    mean= 0.0 # torch.zeros(3)\n    var= 0.0 # torch.zeros(3)\n    n_imgs_total=0\n    i=0\n    for images, _ in dataloader:\n\n        n_imgs_per_batch = images.shape[0]\n#         print(images)\n        #b,c,w*h\n        channelwise_images = images.view(n_imgs_per_batch, images.shape[1], -1)\n        #mean and var per channel, summed every batch\n        mean += channelwise_images.mean(2).sum(0)\n\n        var += channelwise_images.var(2).sum(0)\n\n        n_imgs_total += n_imgs_per_batch\n\n    #means can simply be averaged\n    mean /=n_imgs_total\n\n    #std can't be averaged. but, for equal batch sizes (only one batch might have different size)\n    std = torch.sqrt(var/n_imgs_total)\n    print(n_imgs_total)\n    return mean,std\n","metadata":{"id":"uHTvZjk6N-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_ds_mean_std(train_dataloader)","metadata":{"id":"yjsO2PCwN-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output of above\nds_mean, ds_std = [0.4731, 0.4819, 0.4018], [0.3544, 0.3544, 0.3544]","metadata":{"id":"NvLgGcg6N-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_func = transforms.Compose([transforms.Resize((224,224)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomRotation(10),\n                                      ToTensor(),\n                                      transforms.Normalize(mean=ds_mean, std=ds_std)])\n\ntrain_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\n# test_ds= datasets.ImageFolder(test_dir, transform=transforms_func)\nval_ds= datasets.ImageFolder(val_dir, transform=transforms_func)\n\n#data loaders for training\n\nbatch_size = 25\n\ntrain_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n# test_dataloader = DataLoader(test_ds, batch_size=batch_size,shuffle=False)\nval_dataloader = DataLoader(val_ds, batch_size=batch_size,shuffle=True)","metadata":{"id":"xhH1eQHlN-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for labels,image in train_dataloader:\n#     print(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop","metadata":{"id":"mhBkEXY5N-MS"}},{"cell_type":"code","source":"n_epochs= 10\n#scheduler ni chaine ho? for lr decay. edit: Hola, lets see. TODO: copy paper's\n\noptimizer = torch.optim.RMSprop(model.parameters(), lr=0.256, alpha=0.9, momentum=0.9, weight_decay = 1e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)\ncriterion = nn.CrossEntropyLoss()","metadata":{"id":"EKApTZpWN-MS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=optimizer.param_groups\na[0].keys()\n","metadata":{"scrolled":true,"id":"mvbTQ9bMN-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#required\nn_training_samples =22500","metadata":{"id":"u2WhSFmzN-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=torch.randn(5,25)\na","metadata":{"id":"SuN1Dt-ON-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a.argmax(dim=1, keepdim=True)","metadata":{"id":"sFiHiIA1N-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    #list of param groups\n    return optimizer.param_groups[0][\"lr\"]\n\n# def get_lr(optimizer):\n#     for param_group in optimizer.param_groups:\n#         return param_group[\"lr\"]\n\nprint(get_lr(optimizer))","metadata":{"id":"KMfI1AyUN-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the number of correct predictions per batch\n(5,25)\ndef count_correct_batch(output, target):\n    pred = output.argmax(dim=1, keepdim=True)\n    print(f\"highest pred class: {pred}\")\n    n_correct =pred.eq(target.view_as(pred)).sum().item()\n    return n_correct","metadata":{"id":"8U3ezhi3N-MT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate loss values per batch of data\n\ndef loss_batch(criterion, result, target, optimizer=None):\n\n    #get loss\n    loss=criterion(result,target)\n    print(f\"batch loss:{loss} \")\n    #get performance metric\n    print(f\"result: {result}\")\n    print(f\"target: {target}\")\n    n_correct_b = count_correct_batch(result,target)\n\n    print(f\"n_correct per batch: {n_correct_b}\")\n\n    if optimizer is not None:\n        optimizer.zerograd()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), n_correct_b","metadata":{"id":"WbfxlhrzN-MU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"id":"OwU4kjSBN-MU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_acc_epoch(model, criterion, dataloader, len_data, check_id=False, optimizer=None):\n\n    running_loss =0.0\n    running_n_correct =0.0\n    len_data = len(dataloader)\n\n    for images, labels in tqdm(dataloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        result = model(images)\n\n        loss_b, n_correct_b = loss_batch(criterion,result,labels, optimizer)\n\n        running_loss += loss_b\n\n        if(n_correct_b is not None):\n            running_n_correct += n_correct_b\n\n        if(check_id):\n            break #stop when checking\n\n\n    loss_epoch = running_loss/float(len_data)\n    acc_epoch = running_n_correct/float(len_data)\n\n    return loss_epoch, acc_epoch\n","metadata":{"id":"qb1fFOQvN-MU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy","metadata":{"id":"7lXTMEfQN-MU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epoch, device, scheduler,check_id, save_path, val_iter = 5):\n\n    loss_hist = {\n        \"train\": [],\n        \"val\": []\n    }\n\n    acc_hist={\n        \"train\": [],\n        \"val\": []\n    }\n\n\n    #copy the current model wts as best model:\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss=float('inf')\n    step=0\n    #TODO:\n\n    for epoch in range(n_epochs):\n\n        print(f\"Epoch {epoch+1} of {n_epochs}\")\n        current_lr = get_lr(optimizer)\n        step=0\n\n\n        model.train()\n        train_loss_epoch, train_acc_epoch = loss_acc_epoch(model, criterion, train_dataloader, check_id, optimizer)\n\n        loss_hist[\"train\"].append(train_loss_epoch)\n        acc_hist[\"train\"].append(train_acc_epoch)\n        step+=1\n\n        #evaluation now\n\n        model.eval()\n        with torch.no_grad():\n            val_loss_epoch, val_acc_epoch = loss_acc_epoch(model,criterion, val_dataloader, check_id)\n            loss_hist[\"val\"].append(val_loss_epoch)\n            acc_hist[\"val\"].append(val_acc_epoch)\n\n        #every epoch, keep on selecting the best model sofar\n        if val_loss_epoch < best_loss:\n            best_loss = val_loss_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n            torch.save(model.state_dict(), \"savemodel.pt\")\n            print(\"best model saved\")\n\n        scheduler.step()\n        print(f\"Tr.Loss: {train_loss_epoch:.5f}, Tr.Acc:{train_acc_epoch:.5f} ... Val.loss: {val_loss_epoch}, Val.Acc: {val_acc_epoch:.5f} \")\n\n\n    return loss_hist, acc_hist","metadata":{"id":"6UI9LD5xN-MU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"/kaggle/working/\"","metadata":{"id":"crcAXdE4N-MV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models, transforms","metadata":{"id":"bEoNK7veN-MV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.efficientnet_b5(pretrained=False)\n\n# Modify the classifier to have 6 output classes instead of 1000\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 25)\n\ndef initialize_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, 0, 0.01)\n        nn.init.constant_(m.bias, 0)\n\n# Apply the weight initialization\nmodel.apply(initialize_weights)","metadata":{"scrolled":true,"id":"5ls2piujN-MV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"scrolled":true,"id":"fqT4DFOcN-MY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss_hist, acc_hist = train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epochs=10, device=device, scheduler=scheduler,check_id=False, save_path=save_path,)","metadata":{"id":"SaV_lWd6N-MY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path='model.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(model, criteria, optimizer, n_epochs, train_dataloader, val_dataloader, device, val_iter=10):\n\n#     train_losses=[]\n#     train_accs=[]\n#     val_losses=[]\n#     val_accs=[]\n\n#     for epoch in range(1, n_epochs+1):\n\n#         print(f\"Epoch {epoch} of {n_epochs}:\")\n\n#         #------training------------\n\n#         n_train_samples = len(train_dataloader.dataset)\n\n#         #train mode activate\n#         model.train()\n#         epoch_train_loss = 0\n#         correct_preds=0\n#         epoch_train_acc = float(0)\n#         step=0\n\n\n#         #all the training that happens per epoch\n#         for images, targets in tqdm(train_dataloader):\n#             #iterates through the dataset in batches\n\n#             images = images.to(device)\n#             targets = targets.to(device)\n#             optimizer.zero_grad()\n\n#             #forward pass\n#             results = model(images)\n#             losses = criteria(results, targets.unsqueeze(1).float())\n\n#             #backward pass\n#             losses.backward()\n#             optimizer.step()\n\n#             epoch_train_loss += losses.item()\n#             epoch_train_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n\n#             step +=1\n\n#         #after going through the entire dataset calculate the train loss and accuracy\n#         epoch_train_loss /= step\n#         train_losses.append(epoch_train_loss)\n\n#         epoch_train_acc /= float(n_train_samples)\n#         train_accs.append(epoch_train_acc)\n\n#         print(f\"Epoch {epoch} Training Loss: {epoch_train_loss:.4f}\")\n#         print(f\"Epoch {epoch} Training Acc: {epoch_train_acc:.4f}\")\n\n\n#         #perform validation every few epochs\n#         if epoch % val_iter ==0:\n\n#             #------validation------------\n#             n_val_samples = len(val_dataloader.dataset)\n\n#             #evaluation mode activate\n#             model.eval()\n#             epoch_val_loss =0\n#             epoch_val_acc = float(0)\n#             val_step=0\n\n#             with torch.no_grad():\n#                 for images, targets in tqdm(val_dataloader):\n#                     #iterates through the dataset in batches\n\n#                     images = images.to(device)\n#                     targets = targets.to(device)\n#                     optimizer.zero_grad()\n\n#                     #forward pass\n#                     results = model(images)\n#                     losses = criteria(results, targets.unsqueeze(1).float())\n\n#                     #no backward pass\n\n#                     epoch_val_loss += losses.item()\n\n#                     #rounding basically thresholds the output at 0.5\n#                     epoch_val_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n\n#                     val_step +=1\n\n#                 #after going through the entire dataset calculate the validation loss and accuracy\n#                 epoch_val_loss /= val_step\n#                 val_losses.append(epoch_val_loss)\n#                 epoch_val_acc /= n_val_samples\n#                 val_accs.append(epoch_val_acc)\n\n#         print(f\"Epoch {epoch} Validation Loss: {epoch_train_loss:.4f}\")\n#         print(f\"Epoch {epoch} Validation Acc: {epoch_train_acc:.4f}\")\n\n#     return train_losses, val_losses, train_accs, val_accs","metadata":{"id":"M1YES11SN-MY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport torch\nfrom tqdm import tqdm\n\ndef loss_acc_epoch(model, criterion, dataloader, check_id, device, optimizer=None):\n    epoch_loss = 0\n    correct_preds = 0\n    total_samples = len(dataloader.dataset)\n    steps = 0\n\n    for images, targets in tqdm(dataloader):\n        images = images.to(device)\n        targets = targets.to(device)\n\n        if optimizer:\n            optimizer.zero_grad()\n\n        results = model(images)\n        losses = criterion(results, targets)\n\n        if optimizer:\n            losses.backward()\n            optimizer.step()\n\n        epoch_loss += losses.item()\n        correct_preds += (results.argmax(dim=1) == targets).sum().item()\n        steps += 1\n\n    epoch_loss /= steps\n    epoch_acc = correct_preds / total_samples\n\n    return epoch_loss, epoch_acc\n\ndef train(model, train_dataloader, val_dataloader, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path, val_iter=5):\n    loss_hist = {\"train\": [], \"val\": []}\n    acc_hist = {\"train\": [], \"val\": []}\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = float('inf')\n\n    for epoch in range(1, n_epochs + 1):\n        print(f\"Epoch {epoch} of {n_epochs}\")\n\n        model.train()\n        train_loss_epoch, train_acc_epoch = loss_acc_epoch(model, criterion, train_dataloader, check_id, device, optimizer)\n\n        loss_hist[\"train\"].append(train_loss_epoch)\n        acc_hist[\"train\"].append(train_acc_epoch)\n\n        if epoch % val_iter == 0 or epoch == n_epochs:\n            model.eval()\n            with torch.no_grad():\n                val_loss_epoch, val_acc_epoch = loss_acc_epoch(model, criterion, val_dataloader, check_id, device)\n                loss_hist[\"val\"].append(val_loss_epoch)\n                acc_hist[\"val\"].append(val_acc_epoch)\n\n            if val_loss_epoch < best_loss:\n                best_loss = val_loss_epoch\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), save_path)\n                print(\"Best model saved\")\n\n            print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch:.4f}, Validation Acc: {val_acc_epoch:.4f}\")\n\n        scheduler.step()\n        print(f\"Training Loss: {train_loss_epoch:.4f}, Training Acc: {train_acc_epoch:.4f}\")\n\n    model.load_state_dict(best_model_wts)\n    return loss_hist, acc_hist\n\n# Example usage (make sure to replace variables with actual values):\n# model, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path = ...\n# loss_hist, acc_hist = train(model, train_dataloader, val_dataloader, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path)\n","metadata":{"id":"BR7zfkO1QIZc","trusted":true},"execution_count":null,"outputs":[]}]}