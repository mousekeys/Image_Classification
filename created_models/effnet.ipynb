{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T06:52:25.443733Z","iopub.status.busy":"2024-08-08T06:52:25.443469Z","iopub.status.idle":"2024-08-08T06:52:25.452816Z","shell.execute_reply":"2024-08-08T06:52:25.452230Z","shell.execute_reply.started":"2024-08-08T06:52:25.443706Z"},"id":"18rySelSN-MH","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EPKq_BCUN-MK"},"source":["## EfficientNet Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T06:52:25.457291Z","iopub.status.busy":"2024-08-08T06:52:25.457065Z"},"id":"tq-jFYP8N-ML","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFiXkWrZN-MM","trusted":true},"outputs":[],"source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1,act=True, bias=False):\n","\n","        super().__init__()\n","        # same padding quick mapping:\n","        # k=1 -> p =0, k=3 -> p=1, k=5 -> p=2\n","        padding = kernel_size // 2\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = padding, bias=bias, groups=groups)\n","        self.batch_norm = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.SiLU() if act else nn.Identity()\n","\n","    def forward(self, x):\n","        x= self.conv(x)\n","        x= self.batch_norm(x)\n","        return self.activation(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvBkkVkNN-MM","trusted":true},"outputs":[],"source":["class SqueezeAndExcitationBlock(nn.Module):\n","    def __init__(self, in_channels,reduction_ratio=16): #reduction_ratio to reduce computation, a hyperparameter. take r=16 for balance in complexity and capacity as in SeNet paper\n","        super().__init__()\n","        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n","        self.fc1 = nn.Linear(in_channels , in_channels//reduction_ratio, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(in_channels//reduction_ratio, in_channels, bias =False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","\n","    def forward(self,x):\n","\n","        x_out = self.squeeze(x)\n","        x_out = torch.flatten(x_out,1)\n","        x_out = self.relu(self.fc1(x_out))\n","        x_out = self.sigmoid(self.fc2(x_out))\n","\n","        x_out = x_out[:,:,None,None]\n","\n","        scaled = x * x_out\n","        return scaled\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqopYI_KN-MM","trusted":true},"outputs":[],"source":["# class SqueezeAndExcitationBlockUncheckedLikelyWrong(nn.Module):\n","#     def __init__(self, in_channels, reduction_ratio=16):\n","#         super().__init__()\n","#         self.squeeze = nn.AdaptiveAvgPool2d(1)\n","#         self.excitation = nn.Sequential(\n","#             nn.Linear(in_channels, in_channels // reduction_ratio, bias=False),\n","#             nn.ReLU(inplace=True),\n","#             nn.Linear(in_channels // reduction_ratio, in_channels, bias=False),\n","#             nn.Sigmoid()\n","#         )\n","\n","#     def forward(self, x):\n","#         batch, channels, _, _ = x.size()\n","#         y = self.squeeze(x).view(batch, channels)\n","#         y = self.excitation(y).view(batch, channels, 1, 1)\n","#         return x * y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GdK7iBbN-MN","trusted":true},"outputs":[],"source":["class MobileConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion_ratio, reduction_ratio):\n","        super().__init__()\n","\n","        exp_out_channels = in_channels * expansion_ratio\n","\n","        #residual connection only when it is not being downsampled in any way\n","        self.add_res = in_channels == out_channels and stride ==1\n","        self.conv1 = CNNBlock(in_channels, exp_out_channels, 1,1) if expansion_ratio > 1 else nn.Identity()\n","\n","        #depthwise convolution\n","        self.conv2 = CNNBlock(exp_out_channels, exp_out_channels, kernel_size, stride, exp_out_channels)\n","        self.se = SqueezeAndExcitationBlock(exp_out_channels,reduction_ratio)\n","        self.conv3 = CNNBlock(exp_out_channels,out_channels, 1,1, act=False) #hatched line features means no activation\n","\n","        self.sd = StochasticDepth(0.75)\n","\n","    def forward(self, x):\n","        x_out = self.conv3(self.se(self.conv2(self.conv1(x))))\n","\n","        if self.add_res:\n","            x_out = x + x_out\n","\n","        x_out = self.sd(x_out)\n","\n","        return x_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPSbQMyBN-MN","trusted":true},"outputs":[],"source":["class ClassificationBlock(nn.Module):\n","    def __init__(self,in_channels, n_classes, dropout_prob):\n","        super().__init__()\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.fc = nn.Linear(in_channels, n_classes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self,x):\n","        x = self.avgpool(x)\n","        x = self.dropout(x)\n","        x= torch.flatten(x,1)\n","        x = self.fc(x)\n","        return self.sigmoid(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10pCqBarN-MO","trusted":true},"outputs":[],"source":["class StochasticDepth(nn.Module):\n","    def __init__(self, p=0.75):\n","        super().__init__()\n","        self.p = p\n","\n","    def forward(self, x):\n","\n","        rand_mask = torch.rand((x.shape[0], 1,1,1),  dtype=x.dtype, device=x.device)\n","        binary_mask = torch.floor(rand_mask) #TODO\n","\n","        if self.training:   x = x/self.p * binary_mask\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YBFEHy8N-MO","trusted":true},"outputs":[],"source":["class EffNet(nn.Module):\n","    def __init__(self, model_name, in_channels=3, n_classes=25, show_output_dims=False):\n","        super().__init__()\n","        self.show = show_output_dims\n","        self.model_name = model_name\n","        self.config = Config()\n","        self.stages = self.config.stages\n","        self.phis = self.config.phis[model_name]\n","\n","        #parameters\n","        phi, res, dropout_p = self.phis\n","        self.calc_coeffs(phi)\n","\n","        #define network\n","        self.network = nn.ModuleList([])\n","        self.channels =[]\n","\n","        #baseline stage 1\n","        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[0]\n","        self.add_layers(3, operator, channels, layers, kernel_size, stride) #rgb input layer, TODO: check for errors\n","        print(operator)\n","\n","        #remaining stages: 9 stages ko 7 stages (2-8) lai\n","        for i in range(1, len(self.stages)-1):\n","\n","            if i==1:\n","                reduction_ratio=4\n","            else:\n","                reduction_ratio=24\n","\n","            operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[i]\n","            self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride, expansion_ratio, reduction_ratio)\n","            print(operator)\n","\n","\n","        #final stage: conv1x1 and classifier\n","        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[-1]\n","        self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride) #the conv layer\n","        print(operator)\n","        self.network.append(ClassificationBlock(self.channels[-1], n_classes,dropout_p)) #the classifier block\n","\n","\n","\n","    def forward(self, x):\n","\n","        for stage_num,module in enumerate(self.network):\n","\n","            x= module(x)\n","\n","            shape = x.shape\n","            if self.show: print(f\"shape of stage{stage_num} : {shape}\")\n","\n","        return x\n","\n","\n","    def add_layers(self, in_channels, operator, channels, layers, kernel_size, stride, *args):\n","\n","        channels, layers = self.update_dw(channels, layers)\n","\n","        if layers == 1:\n","            self.network.append(operator(in_channels, channels, kernel_size, stride, *args))\n","        else:\n","            #the first\n","            self.network.append(operator(in_channels, channels, kernel_size, 1, *args))\n","\n","            #the remaining except first and last: works if there are >3 layers\n","            for _ in range(layers-2):\n","                self.network.append(operator(channels, channels,kernel_size, 1, *args))\n","\n","            #final layer with stride dependent on the stage\n","            self.network.append(operator(channels,channels, kernel_size, stride, *args))\n","\n","        self.channels.append(channels)\n","\n","    # for models higher than the basseline:\n","\n","    def calc_coeffs(self, phi, alpha=1.2, beta =1.1): #alpha and beta from EffNet paper, calculated through grid search. We dont use gamma but use the resolution from config\n","        # in every higher model, the channels is multiplied by width (beta^phi) layers is multiplied by depth (alpha^phi)\n","        self.depth = alpha ** phi\n","        self.width = beta ** phi\n","\n","    def update_dw(self,channels, layers):\n","        return int(channels * self.width), int(layers * self.depth)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhvHxjI2N-MP","trusted":true},"outputs":[],"source":["class Config:\n","    stages = [\n","            # [Operator(F), Channels, Layers, Kernel, Stride, Expansion Ratio]\n","            [CNNBlock, 32, 1, 3, 2, 1],\n","            [MobileConvBlock, 16, 1, 3, 1, 1],\n","            [MobileConvBlock, 24, 2, 3, 2, 6],\n","            [MobileConvBlock, 40, 2, 5, 2, 6],\n","            [MobileConvBlock, 80, 3, 3, 2, 6],\n","            [MobileConvBlock, 112, 3, 5, 1, 6],\n","            [MobileConvBlock, 192, 4, 5, 2, 6],\n","            [MobileConvBlock, 320, 1, 3, 1, 6],\n","            [CNNBlock, 1280, 1, 1, 1, 0]\n","    ]\n","\n","    phis = {\n","            # BX : (phi, resolution, dropout)\n","            \"B0\" : (0, 224, 0.2),\n","            \"B1\" : (0.5, 240, 0.2),\n","            \"B2\" : (1, 260, 0.3),\n","            \"B3\" : (2, 300, 0.3),\n","            \"B4\" : (3, 380, 0.4),\n","            \"B5\" : (4, 456, 0.4),\n","            \"B6\" : (5, 528, 0.5),\n","            \"B7\" : (6, 600, 0.5)\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwEF2cAeN-MP","trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name =\"B0\"\n","model = EffNet(model_name = model_name, in_channels = 3, n_classes = 25, show_output_dims=False).to(device)\n","res = Config().phis[model_name][1]\n","print(res)\n","\n","x= torch.randn((5, 3, res, res)).to(device)\n","out= model(x)\n","print(out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QT-5MWj3N-MP","trusted":true},"outputs":[],"source":["!pip install torch-summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xaa63l6cN-MQ","trusted":true},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULc-a_s-N-MR","trusted":true},"outputs":[],"source":["_ =summary(model,x)"]},{"cell_type":"markdown","metadata":{"id":"G8FpwjNNN-MR"},"source":["## 2. Training, Validating and Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwwpBcC1N-MR","trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import transforms, ToTensor\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dADP73m9N-MR","trusted":true},"outputs":[],"source":["train_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/train\"\n","val_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/val\"\n","test_dir =\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0x6J6h0N-MS","trusted":true},"outputs":[],"source":["#data loaders for mean calculation\n","transforms_func = transforms.Compose([transforms.Resize((224,224)),\n","                                      ToTensor()])\n","train_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\n","batch_size = 32\n","train_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHTvZjk6N-MS","trusted":true},"outputs":[],"source":["def get_ds_mean_std(dataloader):\n","    mean= 0.0 # torch.zeros(3)\n","    var= 0.0 # torch.zeros(3)\n","    n_imgs_total=0\n","    i=0\n","    for images, _ in dataloader:\n","\n","        n_imgs_per_batch = images.shape[0]\n","#         print(images)\n","        #b,c,w*h\n","        channelwise_images = images.view(n_imgs_per_batch, images.shape[1], -1)\n","        #mean and var per channel, summed every batch\n","        mean += channelwise_images.mean(2).sum(0)\n","\n","        var += channelwise_images.var(2).sum(0)\n","\n","        n_imgs_total += n_imgs_per_batch\n","\n","    #means can simply be averaged\n","    mean /=n_imgs_total\n","\n","    #std can't be averaged. but, for equal batch sizes (only one batch might have different size)\n","    std = torch.sqrt(var/n_imgs_total)\n","    print(n_imgs_total)\n","    return mean,std\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjsO2PCwN-MS","trusted":true},"outputs":[],"source":["# get_ds_mean_std(train_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvLgGcg6N-MS","trusted":true},"outputs":[],"source":["#output of above\n","ds_mean, ds_std = [0.4731, 0.4819, 0.4018], [0.3544, 0.3544, 0.3544]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhH1eQHlN-MS","trusted":true},"outputs":[],"source":["transforms_func = transforms.Compose([transforms.Resize((224,224)),\n","                                      transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomRotation(10),\n","                                      ToTensor(),\n","                                      transforms.Normalize(mean=ds_mean, std=ds_std)])\n","\n","train_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\n","# test_ds= datasets.ImageFolder(test_dir, transform=transforms_func)\n","val_ds= datasets.ImageFolder(val_dir, transform=transforms_func)\n","\n","#data loaders for training\n","\n","batch_size = 25\n","\n","train_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n","# test_dataloader = DataLoader(test_ds, batch_size=batch_size,shuffle=False)\n","val_dataloader = DataLoader(val_ds, batch_size=batch_size,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for labels,image in train_dataloader:\n","#     print(image)"]},{"cell_type":"markdown","metadata":{"id":"mhBkEXY5N-MS"},"source":["### Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKApTZpWN-MS","trusted":true},"outputs":[],"source":["n_epochs= 10\n","#scheduler ni chaine ho? for lr decay. edit: Hola, lets see. TODO: copy paper's\n","\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=0.256, alpha=0.9, momentum=0.9, weight_decay = 1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvbTQ9bMN-MT","scrolled":true,"trusted":true},"outputs":[],"source":["a=optimizer.param_groups\n","a[0].keys()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2WhSFmzN-MT","trusted":true},"outputs":[],"source":["#required\n","n_training_samples =22500"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuN1Dt-ON-MT","trusted":true},"outputs":[],"source":["a=torch.randn(5,25)\n","a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFiHiIA1N-MT","trusted":true},"outputs":[],"source":["a.argmax(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMfI1AyUN-MT","trusted":true},"outputs":[],"source":["def get_lr(optimizer):\n","    #list of param groups\n","    return optimizer.param_groups[0][\"lr\"]\n","\n","# def get_lr(optimizer):\n","#     for param_group in optimizer.param_groups:\n","#         return param_group[\"lr\"]\n","\n","print(get_lr(optimizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8U3ezhi3N-MT","trusted":true},"outputs":[],"source":["#get the number of correct predictions per batch\n","(5,25)\n","def count_correct_batch(output, target):\n","    pred = output.argmax(dim=1, keepdim=True)\n","    print(f\"highest pred class: {pred}\")\n","    n_correct =pred.eq(target.view_as(pred)).sum().item()\n","    return n_correct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbfxlhrzN-MU","trusted":true},"outputs":[],"source":["#calculate loss values per batch of data\n","\n","def loss_batch(criterion, result, target, optimizer=None):\n","\n","    #get loss\n","    loss=criterion(result,target)\n","    print(f\"batch loss:{loss} \")\n","    #get performance metric\n","    print(f\"result: {result}\")\n","    print(f\"target: {target}\")\n","    n_correct_b = count_correct_batch(result,target)\n","\n","    print(f\"n_correct per batch: {n_correct_b}\")\n","\n","    if optimizer is not None:\n","        optimizer.zerograd()\n","        loss.backward()\n","        opt.step()\n","\n","    return loss.item(), n_correct_b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwU4kjSBN-MU","trusted":true},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qb1fFOQvN-MU","trusted":true},"outputs":[],"source":["def loss_acc_epoch(model, criterion, dataloader, len_data, check_id=False, optimizer=None):\n","\n","    running_loss =0.0\n","    running_n_correct =0.0\n","    len_data = len(dataloader)\n","\n","    for images, labels in tqdm(dataloader):\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        result = model(images)\n","\n","        loss_b, n_correct_b = loss_batch(criterion,result,labels, optimizer)\n","\n","        running_loss += loss_b\n","\n","        if(n_correct_b is not None):\n","            running_n_correct += n_correct_b\n","\n","        if(check_id):\n","            break #stop when checking\n","\n","\n","    loss_epoch = running_loss/float(len_data)\n","    acc_epoch = running_n_correct/float(len_data)\n","\n","    return loss_epoch, acc_epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lXTMEfQN-MU","trusted":true},"outputs":[],"source":["import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UI9LD5xN-MU","trusted":true},"outputs":[],"source":["def train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epoch, device, scheduler,check_id, save_path, val_iter = 5):\n","\n","    loss_hist = {\n","        \"train\": [],\n","        \"val\": []\n","    }\n","\n","    acc_hist={\n","        \"train\": [],\n","        \"val\": []\n","    }\n","\n","\n","    #copy the current model wts as best model:\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss=float('inf')\n","    step=0\n","    #TODO:\n","\n","    for epoch in range(n_epochs):\n","\n","        print(f\"Epoch {epoch+1} of {n_epochs}\")\n","        current_lr = get_lr(optimizer)\n","        step=0\n","\n","\n","        model.train()\n","        train_loss_epoch, train_acc_epoch = loss_acc_epoch(model, criterion, train_dataloader, check_id, optimizer)\n","\n","        loss_hist[\"train\"].append(train_loss_epoch)\n","        acc_hist[\"train\"].append(train_acc_epoch)\n","        step+=1\n","\n","        #evaluation now\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss_epoch, val_acc_epoch = loss_acc_epoch(model,criterion, val_dataloader, check_id)\n","            loss_hist[\"val\"].append(val_loss_epoch)\n","            acc_hist[\"val\"].append(val_acc_epoch)\n","\n","        #every epoch, keep on selecting the best model sofar\n","        if val_loss_epoch < best_loss:\n","            best_loss = val_loss_epoch\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","\n","            torch.save(model.state_dict(), \"savemodel.pt\")\n","            print(\"best model saved\")\n","\n","        scheduler.step()\n","        print(f\"Tr.Loss: {train_loss_epoch:.5f}, Tr.Acc:{train_acc_epoch:.5f} ... Val.loss: {val_loss_epoch}, Val.Acc: {val_acc_epoch:.5f} \")\n","\n","\n","    return loss_hist, acc_hist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crcAXdE4N-MV","trusted":true},"outputs":[],"source":["save_path = \"/kaggle/working/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEoNK7veN-MV","trusted":true},"outputs":[],"source":["from torchvision import models, transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ls2piujN-MV","scrolled":true,"trusted":true},"outputs":[],"source":["model = models.efficientnet_b5(pretrained=False)\n","\n","# Modify the classifier to have 6 output classes instead of 1000\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 25)\n","\n","def initialize_weights(m):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","    elif isinstance(m, nn.BatchNorm2d):\n","        nn.init.constant_(m.weight, 1)\n","        nn.init.constant_(m.bias, 0)\n","    elif isinstance(m, nn.Linear):\n","        nn.init.normal_(m.weight, 0, 0.01)\n","        nn.init.constant_(m.bias, 0)\n","\n","# Apply the weight initialization\n","model.apply(initialize_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqT4DFOcN-MY","scrolled":true,"trusted":true},"outputs":[],"source":["model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaV_lWd6N-MY","trusted":true},"outputs":[],"source":["# loss_hist, acc_hist = train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epochs=10, device=device, scheduler=scheduler,check_id=False, save_path=save_path,)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_path='model.pt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1YES11SN-MY","trusted":true},"outputs":[],"source":["def train(model, criteria, optimizer, n_epochs, train_dataloader, val_dataloader, device, val_iter=10):\n","\n","    train_losses=[]\n","    train_accs=[]\n","    val_losses=[]\n","    val_accs=[]\n","\n","    for epoch in range(1, n_epochs+1):\n","\n","        print(f\"Epoch {epoch} of {n_epochs}:\")\n","\n","        #------training------------\n","\n","        n_train_samples = len(train_dataloader.dataset)\n","\n","        #train mode activate\n","        model.train()\n","        epoch_train_loss = 0\n","        correct_preds=0\n","        epoch_train_acc = float(0)\n","        step=0\n","\n","\n","        #all the training that happens per epoch\n","        for images, targets in tqdm(train_dataloader):\n","            #iterates through the dataset in batches\n","\n","            images = images.to(device)\n","            targets = targets.to(device)\n","            optimizer.zero_grad()\n","\n","            #forward pass\n","            results = model(images)\n","            losses = criteria(results, targets.unsqueeze(1).float())\n","\n","            #backward pass\n","            losses.backward()\n","            optimizer.step()\n","\n","            epoch_train_loss += losses.item()\n","            epoch_train_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n","\n","            step +=1\n","\n","        #after going through the entire dataset calculate the train loss and accuracy\n","        epoch_train_loss /= step\n","        train_losses.append(epoch_train_loss)\n","\n","        epoch_train_acc /= float(n_train_samples)\n","        train_accs.append(epoch_train_acc)\n","\n","        print(f\"Epoch {epoch} Training Loss: {epoch_train_loss:.4f}\")\n","        print(f\"Epoch {epoch} Training Acc: {epoch_train_acc:.4f}\")\n","\n","\n","        #perform validation every few epochs\n","        if epoch % val_iter ==0:\n","\n","            #------validation------------\n","            n_val_samples = len(val_dataloader.dataset)\n","\n","            #evaluation mode activate\n","            model.eval()\n","            epoch_val_loss =0\n","            epoch_val_acc = float(0)\n","            val_step=0\n","\n","            with torch.no_grad():\n","                for images, targets in tqdm(val_dataloader):\n","                    #iterates through the dataset in batches\n","\n","                    images = images.to(device)\n","                    targets = targets.to(device)\n","                    optimizer.zero_grad()\n","\n","                    #forward pass\n","                    results = model(images)\n","                    losses = criteria(results, targets.unsqueeze(1).float())\n","\n","                    #no backward pass\n","\n","                    epoch_val_loss += losses.item()\n","\n","                    #rounding basically thresholds the output at 0.5\n","                    epoch_val_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n","\n","                    val_step +=1\n","\n","                #after going through the entire dataset calculate the validation loss and accuracy\n","                epoch_val_loss /= val_step\n","                val_losses.append(epoch_val_loss)\n","                epoch_val_acc /= n_val_samples\n","                val_accs.append(epoch_val_acc)\n","\n","        print(f\"Epoch {epoch} Validation Loss: {epoch_train_loss:.4f}\")\n","        print(f\"Epoch {epoch} Validation Acc: {epoch_train_acc:.4f}\")\n","\n","    return train_losses, val_losses, train_accs, val_accs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR7zfkO1QIZc","trusted":true},"outputs":[],"source":["import copy\n","import torch\n","from tqdm import tqdm\n","\n","def loss_acc_epoch(model, criterion, dataloader, check_id, device, optimizer=None):\n","    epoch_loss = 0\n","    correct_preds = 0\n","    total_samples = len(dataloader.dataset)\n","    steps = 0\n","\n","    for images, targets in tqdm(dataloader):\n","        images = images.to(device)\n","        targets = targets.to(device)\n","\n","        if optimizer:\n","            optimizer.zero_grad()\n","\n","        results = model(images)\n","        losses = criterion(results, targets)\n","\n","        if optimizer:\n","            losses.backward()\n","            optimizer.step()\n","\n","        epoch_loss += losses.item()\n","        correct_preds += (results.argmax(dim=1) == targets).sum().item()\n","        steps += 1\n","\n","    epoch_loss /= steps\n","    epoch_acc = correct_preds / total_samples\n","\n","    return epoch_loss, epoch_acc\n","\n","def train(model, train_dataloader, val_dataloader, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path, val_iter=5):\n","    loss_hist = {\"train\": [], \"val\": []}\n","    acc_hist = {\"train\": [], \"val\": []}\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = float('inf')\n","\n","    for epoch in range(1, n_epochs + 1):\n","        print(f\"Epoch {epoch} of {n_epochs}\")\n","\n","        model.train()\n","        train_loss_epoch, train_acc_epoch = loss_acc_epoch(model, criterion, train_dataloader, check_id, device, optimizer)\n","\n","        loss_hist[\"train\"].append(train_loss_epoch)\n","        acc_hist[\"train\"].append(train_acc_epoch)\n","\n","        if epoch % val_iter == 0 or epoch == n_epochs:\n","            model.eval()\n","            with torch.no_grad():\n","                val_loss_epoch, val_acc_epoch = loss_acc_epoch(model, criterion, val_dataloader, check_id, device)\n","                loss_hist[\"val\"].append(val_loss_epoch)\n","                acc_hist[\"val\"].append(val_acc_epoch)\n","\n","            if val_loss_epoch < best_loss:\n","                best_loss = val_loss_epoch\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                torch.save(model.state_dict(), save_path)\n","                print(\"Best model saved\")\n","\n","            print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch:.4f}, Validation Acc: {val_acc_epoch:.4f}\")\n","\n","        scheduler.step()\n","        print(f\"Training Loss: {train_loss_epoch:.4f}, Training Acc: {train_acc_epoch:.4f}\")\n","\n","    model.load_state_dict(best_model_wts)\n","    return loss_hist, acc_hist\n","\n","# Example usage (make sure to replace variables with actual values):\n","# model, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path = ...\n","# loss_hist, acc_hist = train(model, train_dataloader, val_dataloader, criterion, optimizer, n_epochs, device, scheduler, check_id, save_path)\n"]}],"metadata":{"colab":{"name":"PXL_classification","provenance":[]},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":5490352,"sourceId":9097401,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
