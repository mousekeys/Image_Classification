{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9097401,"sourceType":"datasetVersion","datasetId":5490352}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## EfficientNet Architecture","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:51.655930Z","iopub.execute_input":"2024-08-08T13:15:51.657232Z","iopub.status.idle":"2024-08-08T13:15:51.663091Z","shell.execute_reply.started":"2024-08-08T13:15:51.657190Z","shell.execute_reply":"2024-08-08T13:15:51.661501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1,act=True, bias=False):\n        \n        super().__init__()\n        # same padding quick mapping:\n        # k=1 -> p =0, k=3 -> p=1, k=5 -> p=2\n        padding = kernel_size // 2\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = padding, bias=bias, groups=groups)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.activation = nn.SiLU() if act else nn.Identity()\n        \n    def forward(self, x):\n        x= self.conv(x)\n        x= self.batch_norm(x)\n        return self.activation(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:52.017186Z","iopub.execute_input":"2024-08-08T13:15:52.017636Z","iopub.status.idle":"2024-08-08T13:15:52.027584Z","shell.execute_reply.started":"2024-08-08T13:15:52.017602Z","shell.execute_reply":"2024-08-08T13:15:52.026260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SqueezeAndExcitationBlock(nn.Module):\n    def __init__(self, in_channels,reduction_ratio=16): #reduction_ratio to reduce computation, a hyperparameter. take r=16 for balance in complexity and capacity as in SeNet paper\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n        self.fc1 = nn.Linear(in_channels , in_channels//reduction_ratio, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(in_channels//reduction_ratio, in_channels, bias =False)\n        self.sigmoid = nn.Sigmoid()\n        \n        \n    def forward(self,x):\n        \n        x_out = self.squeeze(x)\n        x_out = torch.flatten(x_out,1)\n        x_out = self.relu(self.fc1(x_out))\n        x_out = self.sigmoid(self.fc2(x_out))\n        \n        x_out = x_out[:,:,None,None]\n        \n        scaled = x * x_out\n        return scaled\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:52.663169Z","iopub.execute_input":"2024-08-08T13:15:52.663618Z","iopub.status.idle":"2024-08-08T13:15:52.674677Z","shell.execute_reply.started":"2024-08-08T13:15:52.663583Z","shell.execute_reply":"2024-08-08T13:15:52.673366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class SqueezeAndExcitationBlockUncheckedLikelyWrong(nn.Module):\n#     def __init__(self, in_channels, reduction_ratio=16):\n#         super().__init__()\n#         self.squeeze = nn.AdaptiveAvgPool2d(1)\n#         self.excitation = nn.Sequential(\n#             nn.Linear(in_channels, in_channels // reduction_ratio, bias=False),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(in_channels // reduction_ratio, in_channels, bias=False),\n#             nn.Sigmoid()\n#         )\n\n#     def forward(self, x):\n#         batch, channels, _, _ = x.size()\n#         y = self.squeeze(x).view(batch, channels)\n#         y = self.excitation(y).view(batch, channels, 1, 1)\n#         return x * y\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:53.189368Z","iopub.execute_input":"2024-08-08T13:15:53.189815Z","iopub.status.idle":"2024-08-08T13:15:53.196730Z","shell.execute_reply.started":"2024-08-08T13:15:53.189781Z","shell.execute_reply":"2024-08-08T13:15:53.195360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MobileConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion_ratio, reduction_ratio):\n        super().__init__()\n        \n        exp_out_channels = in_channels * expansion_ratio\n        \n        #residual connection only when it is not being downsampled in any way\n        self.add_res = in_channels == out_channels and stride ==1\n        self.conv1 = CNNBlock(in_channels, exp_out_channels, 1,1) if expansion_ratio > 1 else nn.Identity()\n        \n        #depthwise convolution\n        self.conv2 = CNNBlock(exp_out_channels, exp_out_channels, kernel_size, stride, exp_out_channels)\n        self.se = SqueezeAndExcitationBlock(exp_out_channels,reduction_ratio)\n        self.conv3 = CNNBlock(exp_out_channels,out_channels, 1,1, act=False) #hatched line features means no activation\n        \n        self.sd = StochasticDepth(0.75)\n        \n    def forward(self, x):\n        x_out = self.conv3(self.se(self.conv2(self.conv1(x))))\n        \n        if self.add_res:\n            x_out = x + x_out\n        \n        x_out = self.sd(x_out)\n        \n        return x_out","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:53.565179Z","iopub.execute_input":"2024-08-08T13:15:53.565653Z","iopub.status.idle":"2024-08-08T13:15:53.577842Z","shell.execute_reply.started":"2024-08-08T13:15:53.565618Z","shell.execute_reply":"2024-08-08T13:15:53.576273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationBlock(nn.Module):\n    def __init__(self,in_channels, n_classes, dropout_prob):\n        super().__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc = nn.Linear(in_channels, n_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self,x):\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x= torch.flatten(x,1)\n        x = self.fc(x)\n        return self.sigmoid(x)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:53.978759Z","iopub.execute_input":"2024-08-08T13:15:53.979698Z","iopub.status.idle":"2024-08-08T13:15:53.988384Z","shell.execute_reply.started":"2024-08-08T13:15:53.979659Z","shell.execute_reply":"2024-08-08T13:15:53.987010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StochasticDepth(nn.Module):\n    def __init__(self, p=0.75):\n        super().__init__()\n        self.p = p\n    \n    def forward(self, x):\n        \n        rand_mask = torch.rand((x.shape[0], 1,1,1),  dtype=x.dtype, device=x.device)\n        binary_mask = torch.floor(rand_mask) #TODO\n      \n        if self.training:   x = x/self.p * binary_mask\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:54.432749Z","iopub.execute_input":"2024-08-08T13:15:54.433226Z","iopub.status.idle":"2024-08-08T13:15:54.441858Z","shell.execute_reply.started":"2024-08-08T13:15:54.433195Z","shell.execute_reply":"2024-08-08T13:15:54.440484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EffNet(nn.Module):\n    def __init__(self, model_name, in_channels=3, n_classes=25, show_output_dims=False):\n        super().__init__()\n        self.show = show_output_dims\n        self.model_name = model_name\n        self.config = Config()\n        self.stages = self.config.stages\n        self.phis = self.config.phis[model_name]\n        \n        #parameters\n        phi, res, dropout_p = self.phis\n        self.calc_coeffs(phi)\n        \n        #define network\n        self.network = nn.ModuleList([])\n        self.channels =[]\n        \n        #baseline stage 1\n        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[0]\n        self.add_layers(3, operator, channels, layers, kernel_size, stride) #rgb input layer, TODO: check for errors\n        print(operator)\n        \n        #remaining stages: 9 stages ko 7 stages (2-8) lai\n        for i in range(1, len(self.stages)-1):\n           \n            if i==1:\n                reduction_ratio=4\n            else:\n                reduction_ratio=24\n                \n            operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[i]\n            self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride, expansion_ratio, reduction_ratio)\n            print(operator)\n               \n\n        #final stage: conv1x1 and classifier\n        operator, channels, layers, kernel_size, stride, expansion_ratio = self.config.stages[-1]\n        self.add_layers(self.channels[-1], operator, channels, layers, kernel_size, stride) #the conv layer\n        print(operator)\n        self.network.append(ClassificationBlock(self.channels[-1], n_classes,dropout_p)) #the classifier block\n\n        \n    \n    def forward(self, x):\n        \n        for stage_num,module in enumerate(self.network):\n            \n            x= module(x)\n\n            shape = x.shape\n            if self.show: print(f\"shape of stage{stage_num} : {shape}\")\n                \n        return x\n    \n    \n    def add_layers(self, in_channels, operator, channels, layers, kernel_size, stride, *args):\n        \n        channels, layers = self.update_dw(channels, layers)\n        \n        if layers == 1:\n            self.network.append(operator(in_channels, channels, kernel_size, stride, *args))\n        else:\n            #the first \n            self.network.append(operator(in_channels, channels, kernel_size, 1, *args))\n            \n            #the remaining except first and last: works if there are >3 layers\n            for _ in range(layers-2):\n                self.network.append(operator(channels, channels,kernel_size, 1, *args))\n            \n            #final layer with stride dependent on the stage\n            self.network.append(operator(channels,channels, kernel_size, stride, *args))\n        \n        self.channels.append(channels)\n\n    # for models higher than the basseline:\n    \n    def calc_coeffs(self, phi, alpha=1.2, beta =1.1): #alpha and beta from EffNet paper, calculated through grid search. We dont use gamma but use the resolution from config\n        # in every higher model, the channels is multiplied by width (beta^phi) layers is multiplied by depth (alpha^phi)\n        self.depth = alpha ** phi\n        self.width = beta ** phi\n    \n    def update_dw(self,channels, layers):\n        return int(channels * self.width), int(layers * self.depth)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:54.820975Z","iopub.execute_input":"2024-08-08T13:15:54.821385Z","iopub.status.idle":"2024-08-08T13:15:54.884350Z","shell.execute_reply.started":"2024-08-08T13:15:54.821354Z","shell.execute_reply":"2024-08-08T13:15:54.882563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    stages = [\n            # [Operator(F), Channels, Layers, Kernel, Stride, Expansion Ratio]\n            [CNNBlock, 32, 1, 3, 2, 1], \n            [MobileConvBlock, 16, 1, 3, 1, 1],\n            [MobileConvBlock, 24, 2, 3, 2, 6],\n            [MobileConvBlock, 40, 2, 5, 2, 6],\n            [MobileConvBlock, 80, 3, 3, 2, 6],\n            [MobileConvBlock, 112, 3, 5, 1, 6],\n            [MobileConvBlock, 192, 4, 5, 2, 6],\n            [MobileConvBlock, 320, 1, 3, 1, 6],\n            [CNNBlock, 1280, 1, 1, 1, 0]\n    ]\n\n    phis = {\n            # BX : (phi, resolution, dropout) \n            \"B0\" : (0, 224, 0.2),\n            \"B1\" : (0.5, 240, 0.2),\n            \"B2\" : (1, 260, 0.3),\n            \"B3\" : (2, 300, 0.3),\n            \"B4\" : (3, 380, 0.4),\n            \"B5\" : (4, 456, 0.4),\n            \"B6\" : (5, 528, 0.5),\n            \"B7\" : (6, 600, 0.5)\n    }","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:15:55.361283Z","iopub.execute_input":"2024-08-08T13:15:55.361756Z","iopub.status.idle":"2024-08-08T13:15:55.373139Z","shell.execute_reply.started":"2024-08-08T13:15:55.361721Z","shell.execute_reply":"2024-08-08T13:15:55.371715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name =\"B0\"\nmodel = EffNet(model_name = model_name, in_channels = 3, n_classes = 25, show_output_dims=False).to(device)\nres = Config().phis[model_name][1]\n\nx= torch.randn((5, 3, res, res)).to(device)\nout= model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:10.691991Z","iopub.execute_input":"2024-08-08T13:16:10.693040Z","iopub.status.idle":"2024-08-08T13:16:11.206115Z","shell.execute_reply.started":"2024-08-08T13:16:10.692996Z","shell.execute_reply":"2024-08-08T13:16:11.204769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:11.828434Z","iopub.execute_input":"2024-08-08T13:16:11.828847Z","iopub.status.idle":"2024-08-08T13:16:27.352690Z","shell.execute_reply.started":"2024-08-08T13:16:11.828811Z","shell.execute_reply":"2024-08-08T13:16:27.351004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:27.355718Z","iopub.execute_input":"2024-08-08T13:16:27.356275Z","iopub.status.idle":"2024-08-08T13:16:27.363751Z","shell.execute_reply.started":"2024-08-08T13:16:27.356223Z","shell.execute_reply":"2024-08-08T13:16:27.362161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _ =summary(model,x)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-08T13:16:33.595037Z","iopub.execute_input":"2024-08-08T13:16:33.595511Z","iopub.status.idle":"2024-08-08T13:16:33.601151Z","shell.execute_reply.started":"2024-08-08T13:16:33.595478Z","shell.execute_reply":"2024-08-08T13:16:33.599631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Training, Validating and Saving the model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms, ToTensor\nfrom tqdm import tqdm\nfrom torchvision import models","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:34.719551Z","iopub.execute_input":"2024-08-08T13:16:34.720010Z","iopub.status.idle":"2024-08-08T13:16:34.728494Z","shell.execute_reply.started":"2024-08-08T13:16:34.719966Z","shell.execute_reply":"2024-08-08T13:16:34.727095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/train\"\nval_dir = \"/kaggle/input/seen-dataset-12-class/Seen Datasets/val\"\ntest_dir =\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:35.319478Z","iopub.execute_input":"2024-08-08T13:16:35.319929Z","iopub.status.idle":"2024-08-08T13:16:35.325988Z","shell.execute_reply.started":"2024-08-08T13:16:35.319895Z","shell.execute_reply":"2024-08-08T13:16:35.324702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #data loaders for mean calculation\n# transforms_func = transforms.Compose([transforms.Resize((224,224)),\n#                                       ToTensor()])\n# train_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\n# batch_size = 32\n# train_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:36.168223Z","iopub.execute_input":"2024-08-08T13:16:36.168680Z","iopub.status.idle":"2024-08-08T13:16:36.174378Z","shell.execute_reply.started":"2024-08-08T13:16:36.168645Z","shell.execute_reply":"2024-08-08T13:16:36.172806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_ds_mean_std(dataloader):\n#     mean= 0.0 # torch.zeros(3)\n#     var= 0.0 # torch.zeros(3)\n#     n_imgs_total=0\n#     i=0\n#     for images, _ in dataloader:\n        \n#         n_imgs_per_batch = images.shape[0]\n# #         print(images)\n#         #b,c,w*h\n#         channelwise_images = images.view(n_imgs_per_batch, images.shape[1], -1)\n#         #mean and var per channel, summed every batch\n#         mean += channelwise_images.mean(2).sum(0)\n      \n#         var += channelwise_images.var(2).sum(0)\n       \n#         n_imgs_total += n_imgs_per_batch\n            \n#     #means can simply be averaged\n#     mean /=n_imgs_total\n\n#     #std can't be averaged. but, for equal batch sizes (only one batch might have different size)\n#     std = torch.sqrt(var/n_imgs_total)\n#     print(n_imgs_total)\n#     return mean,std\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:37.166842Z","iopub.execute_input":"2024-08-08T13:16:37.167254Z","iopub.status.idle":"2024-08-08T13:16:37.173811Z","shell.execute_reply.started":"2024-08-08T13:16:37.167225Z","shell.execute_reply":"2024-08-08T13:16:37.172328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_ds_mean_std(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:38.182569Z","iopub.execute_input":"2024-08-08T13:16:38.183020Z","iopub.status.idle":"2024-08-08T13:16:38.188241Z","shell.execute_reply.started":"2024-08-08T13:16:38.182986Z","shell.execute_reply":"2024-08-08T13:16:38.186976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output of above\nds_mean, ds_std = [0.4731, 0.4819, 0.4018], [0.3544, 0.3544, 0.3544]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:44.120325Z","iopub.execute_input":"2024-08-08T13:16:44.120758Z","iopub.status.idle":"2024-08-08T13:16:44.127088Z","shell.execute_reply.started":"2024-08-08T13:16:44.120725Z","shell.execute_reply":"2024-08-08T13:16:44.125546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_func = transforms.Compose([transforms.Resize((416,416)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomRotation(10),\n                                      ToTensor(),\n                                      transforms.Normalize(mean=ds_mean, std=ds_std)])\n\ntrain_ds= datasets.ImageFolder(train_dir, transform=transforms_func)\n# test_ds= datasets.ImageFolder(test_dir, transform=transforms_func)\nval_ds= datasets.ImageFolder(val_dir, transform=transforms_func)\n\n#data loaders for training\n\nbatch_size = 32\n\ntrain_dataloader = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n# test_dataloader = DataLoader(test_ds, batch_size=batch_size,shuffle=False)\nval_dataloader = DataLoader(val_ds, batch_size=batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:44.812886Z","iopub.execute_input":"2024-08-08T13:16:44.813300Z","iopub.status.idle":"2024-08-08T13:16:50.813162Z","shell.execute_reply.started":"2024-08-08T13:16:44.813269Z","shell.execute_reply":"2024-08-08T13:16:50.811972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop","metadata":{}},{"cell_type":"code","source":"n_epochs= 15\n#scheduler ni chaine ho? for lr decay. edit: Hola, lets see. TODO: copy paper's\n\noptimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9, momentum=0.9, weight_decay = 1e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5) \ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:52.725780Z","iopub.execute_input":"2024-08-08T13:16:52.727058Z","iopub.status.idle":"2024-08-08T13:16:52.738192Z","shell.execute_reply.started":"2024-08-08T13:16:52.727007Z","shell.execute_reply":"2024-08-08T13:16:52.736689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#required\nn_training_samples =22500","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-08T13:16:53.782855Z","iopub.execute_input":"2024-08-08T13:16:53.783712Z","iopub.status.idle":"2024-08-08T13:16:53.788347Z","shell.execute_reply.started":"2024-08-08T13:16:53.783676Z","shell.execute_reply":"2024-08-08T13:16:53.787247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    #list of param groups\n    return optimizer.param_groups[0][\"lr\"]\n    \n# def get_lr(optimizer):\n#     for param_group in optimizer.param_groups:\n#         return param_group[\"lr\"]\n\nprint(get_lr(optimizer))","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:54.903539Z","iopub.execute_input":"2024-08-08T13:16:54.903963Z","iopub.status.idle":"2024-08-08T13:16:54.911679Z","shell.execute_reply.started":"2024-08-08T13:16:54.903917Z","shell.execute_reply":"2024-08-08T13:16:54.910119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the number of correct predictions per batch\n(5,25)\ndef count_correct_batch(output, target):\n    pred = output.argmax(dim=1, keepdim=True)\n#     print(f\"highest pred class: {pred}\")\n    n_correct =pred.eq(target.view_as(pred)).sum().item()\n    \n    return n_correct","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:56.446254Z","iopub.execute_input":"2024-08-08T13:16:56.446672Z","iopub.status.idle":"2024-08-08T13:16:56.454004Z","shell.execute_reply.started":"2024-08-08T13:16:56.446640Z","shell.execute_reply":"2024-08-08T13:16:56.452533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate loss values per batch of data\n\ndef loss_batch(criterion, result, target, optimizer=None):\n    \n    #get loss\n    loss=criterion(result,target)\n#     print(f\"batch loss:{loss} \")\n    #get performance metric\n#     print(f\"result: {result}\")\n#     print(f\"target: {target.view_as(result.argmax(dim=1, keepdim=True))}\")\n    n_correct_b = count_correct_batch(result,target)\n    \n#     print(f\"n_correct per batch: {n_correct_b}\")\n    \n    if optimizer is not None:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    return loss.item(), n_correct_b","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:57.850373Z","iopub.execute_input":"2024-08-08T13:16:57.850808Z","iopub.status.idle":"2024-08-08T13:16:57.859408Z","shell.execute_reply.started":"2024-08-08T13:16:57.850775Z","shell.execute_reply":"2024-08-08T13:16:57.857162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:58.789890Z","iopub.execute_input":"2024-08-08T13:16:58.790339Z","iopub.status.idle":"2024-08-08T13:16:58.796531Z","shell.execute_reply.started":"2024-08-08T13:16:58.790307Z","shell.execute_reply":"2024-08-08T13:16:58.794937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_batch(batch):\n  \"\"\"Displays images in a batch.\n  Args:\n    batch: A batch of images, either as a NumPy array or PyTorch tensor.\n  \"\"\"\n\n  # Convert to NumPy array if necessary\n  if isinstance(batch, torch.Tensor):\n    batch = batch.numpy()\n\n  # Assuming images are in CHW format, convert to HWC for display\n  batch = np.transpose(batch, (0, 2, 3, 1))\n\n  # Calculate the number of rows and columns for the grid\n  num_images = batch.shape[0]\n  cols = min(8, num_images)\n  rows = (num_images + cols - 1) // cols\n\n  # Create a figure and subplots\n  fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n\n  # Iterate over images and display them\n  for i in range(num_images):\n    ax = axes.flatten()[i]\n    ax.imshow(batch[i])\n    ax.axis('off')\n\n  plt.tight_layout()\n  plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:16:59.520260Z","iopub.execute_input":"2024-08-08T13:16:59.520785Z","iopub.status.idle":"2024-08-08T13:16:59.532024Z","shell.execute_reply.started":"2024-08-08T13:16:59.520748Z","shell.execute_reply":"2024-08-08T13:16:59.530507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_acc_epoch(model, criterion, dataloader,is_train, check_id=False, optimizer=None):\n    \n    running_loss =0.0\n    running_n_correct =0.0\n    \n    n_batches_dataloder = len(dataloader)\n    total_samples = len(dataloader.dataset)\n    \n    batch_number =0\n    for images, labels in tqdm(dataloader):\n        batch_number += 1\n        \n#         if batch_number % 100 ==0:\n#             display_batch(images)\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        result = model(images)\n        \n        loss_b, n_correct_b = loss_batch(criterion,result,labels, optimizer)\n        \n        print(\"n_correct per batch, of 32 preds: \", n_correct_b)\n        \n        running_loss += loss_b\n        \n        if(n_correct_b is not None):\n            running_n_correct += n_correct_b\n        \n        if(check_id):\n            break #stop when checking\n        \n        total_samples += labels.size(0)\n            \n    \n    print(\"running loss epoch at end: \", running_loss)\n    print(\"running n correct at end: \", sunning_n_correct)\n    loss_epoch = running_loss/float(total_samples)\n    acc_epoch = running_n_correct/float(total_samples)\n    \n    return loss_epoch, acc_epoch\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:10.795739Z","iopub.execute_input":"2024-08-08T13:19:10.796213Z","iopub.status.idle":"2024-08-08T13:19:10.808328Z","shell.execute_reply.started":"2024-08-08T13:19:10.796181Z","shell.execute_reply":"2024-08-08T13:19:10.807038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:12.113202Z","iopub.execute_input":"2024-08-08T13:19:12.113624Z","iopub.status.idle":"2024-08-08T13:19:12.119173Z","shell.execute_reply.started":"2024-08-08T13:19:12.113591Z","shell.execute_reply":"2024-08-08T13:19:12.117776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epoch, device, scheduler,check_id, save_path, val_iter = 5):\n    \n    loss_hist = {\n        \"train\": [],\n        \"val\": []\n    }\n    \n    acc_hist={\n        \"train\": [],\n        \"val\": []\n    }\n    \n    \n    #copy the current model wts as best model:\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss=float('inf')\n    #TODO:\n    \n    for epoch in range(n_epochs):\n        \n        print(f\"Epoch {epoch+1} of {n_epochs}\")\n        current_lr = get_lr(optimizer)\n        \n        print(\"training\")\n        model.train()\n        train_loss_epoch, train_acc_epoch = loss_acc_epoch(model, criterion, train_dataloader,True, check_id, optimizer)\n        \n        loss_hist[\"train\"].append(train_loss_epoch)\n        acc_hist[\"train\"].append(train_acc_epoch)\n        print(f\"Tr.Loss: {train_loss_epoch:.5f}, Tr.Acc:{train_acc_epoch:.5f}\")\n        #validating now\n        print(\"validation\")\n        \n        if epoch % 2 ==0:\n            model.eval()\n            with torch.no_grad():\n                val_loss_epoch, val_acc_epoch = loss_acc_epoch(model,criterion, val_dataloader, False, check_id)\n                loss_hist[\"val\"].append(val_loss_epoch)\n                acc_hist[\"val\"].append(val_acc_epoch)\n\n            #every epoch, keep on selecting the best model sofar\n            if val_loss_epoch < best_loss:\n                best_loss = val_loss_epoch\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n                torch.save(model.state_dict(), \"savemodel.pt\")\n                print(\"best model saved\")\n            \n#         scheduler.step()\n#         print(f\"Val.loss: {val_loss_epoch}, Val.Acc: {val_acc_epoch:.5f} \")\n        \n        \n    return loss_hist, acc_hist","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:12.414387Z","iopub.execute_input":"2024-08-08T13:19:12.414847Z","iopub.status.idle":"2024-08-08T13:19:12.428630Z","shell.execute_reply.started":"2024-08-08T13:19:12.414796Z","shell.execute_reply":"2024-08-08T13:19:12.427176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:12.928660Z","iopub.execute_input":"2024-08-08T13:19:12.929098Z","iopub.status.idle":"2024-08-08T13:19:12.935251Z","shell.execute_reply.started":"2024-08-08T13:19:12.929066Z","shell.execute_reply":"2024-08-08T13:19:12.933600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model2 = models.efficientnet_b5(pretrained=False)\n\n# # Modify the classifier to have 6 output classes instead of 1000\n# model2.classifier[1] = nn.Linear(model2.classifier[1].in_features, 25)\n\n# def initialize_weights(m):\n#     if isinstance(m, nn.Conv2d):\n#         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#         if m.bias is not None:\n#             nn.init.constant_(m.bias, 0)\n#     elif isinstance(m, nn.BatchNorm2d):\n#         nn.init.constant_(m.weight, 1)\n#         nn.init.constant_(m.bias, 0)\n#     elif isinstance(m, nn.Linear):\n#         nn.init.normal_(m.weight, 0, 0.01)\n#         nn.init.constant_(m.bias, 0)\n\n# # Apply the weight initialization\n# model2.apply(initialize_weights)\n# model2.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-08T13:19:13.385756Z","iopub.execute_input":"2024-08-08T13:19:13.387064Z","iopub.status.idle":"2024-08-08T13:19:13.392767Z","shell.execute_reply.started":"2024-08-08T13:19:13.387024Z","shell.execute_reply":"2024-08-08T13:19:13.391283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=25):\n        super().__init__()\n        self.features_extraction = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),  \n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n              \n            \n            nn.Conv2d(64, 192, kernel_size=5, padding=2),  \n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(192),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            \n            \n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(384),\n            \n            nn.Conv2d(384, 256, kernel_size=3, padding=1),  \n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            \n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.AdaptiveAvgPool2d((6, 6)),  \n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(4096),\n            \n            nn.Dropout(0.1),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(4096),\n            \n            nn.Dropout(0.1),\n            nn.Linear(4096, 512),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(512),\n            \n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features_extraction(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n    \n\nmodel=AlexNet()\n# model = nn.DataParallel(model, device_ids = [0,1]) # to use multipleGPU\nmodel = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:13.822224Z","iopub.execute_input":"2024-08-08T13:19:13.822921Z","iopub.status.idle":"2024-08-08T13:19:14.447064Z","shell.execute_reply.started":"2024-08-08T13:19:13.822885Z","shell.execute_reply":"2024-08-08T13:19:14.445963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=AlexNet()\n# model = nn.DataParallel(model, device_ids = [0,1]) # to use multipleGPU\nmodel = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:14.448880Z","iopub.execute_input":"2024-08-08T13:19:14.449276Z","iopub.status.idle":"2024-08-08T13:19:15.114246Z","shell.execute_reply.started":"2024-08-08T13:19:14.449244Z","shell.execute_reply":"2024-08-08T13:19:15.113033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist, acc_hist = train(model,train_dataloader, val_dataloader, criterion, optimizer, n_epoch=10, device=device, scheduler=scheduler,check_id=False, save_path=save_path,)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-08T13:19:15.116883Z","iopub.execute_input":"2024-08-08T13:19:15.117384Z","iopub.status.idle":"2024-08-08T13:19:27.892241Z","shell.execute_reply.started":"2024-08-08T13:19:15.117344Z","shell.execute_reply":"2024-08-08T13:19:27.890251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(loss_hist)\nprint(acc_hist)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:27.893689Z","iopub.status.idle":"2024-08-08T13:19:27.894204Z","shell.execute_reply.started":"2024-08-08T13:19:27.893955Z","shell.execute_reply":"2024-08-08T13:19:27.893997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(model, criteria, optimizer, n_epochs, train_dataloader, val_dataloader, device, val_iter=10):\n    \n#     train_losses=[]\n#     train_accs=[]\n#     val_losses=[]\n#     val_accs=[]\n    \n#     for epoch in range(1, n_epochs+1):\n        \n#         print(f\"Epoch {epoch} of {n_epochs}:\")\n        \n#         #------training------------\n        \n#         n_train_samples = len(train_dataloader.dataset)\n        \n#         #train mode activate\n#         model.train()\n#         epoch_train_loss = 0\n#         correct_preds=0\n#         epoch_train_acc = float(0)\n#         step=0\n        \n        \n#         #all the training that happens per epoch\n#         for images, targets in tqdm(train_dataloader):\n#             #iterates through the dataset in batches\n            \n#             images = images.to(device)\n#             targets = targets.to(device)\n#             optimizer.zero_grad()\n            \n#             #forward pass\n#             results = model(images)\n#             losses = criteria(results, targets.unsqueeze(1).float())\n            \n#             #backward pass\n#             losses.backward()\n#             optimizer.step()\n            \n#             epoch_train_loss += losses.item()\n#             epoch_train_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n            \n#             step +=1\n        \n#         #after going through the entire dataset calculate the train loss and accuracy\n#         epoch_train_loss /= step\n#         train_losses.append(epoch_train_loss)\n        \n#         epoch_train_acc /= float(n_train_samples)\n#         train_accs.append(epoch_train_acc)\n        \n#         print(f\"Epoch {epoch} Training Loss: {epoch_train_loss:.4f}\")\n#         print(f\"Epoch {epoch} Training Acc: {epoch_train_acc:.4f}\")\n        \n        \n#         #perform validation every few epochs\n#         if epoch % val_iter ==0:\n            \n#             #------validation------------\n#             n_val_samples = len(val_dataloader.dataset)\n            \n#             #evaluation mode activate\n#             model.eval()\n#             epoch_val_loss =0\n#             epoch_val_acc = float(0)\n#             val_step=0\n            \n#             with torch.no_grad():\n#                 for images, targets in tqdm(val_dataloader):\n#                     #iterates through the dataset in batches\n                    \n#                     images = images.to(device)\n#                     targets = targets.to(device)\n#                     optimizer.zero_grad()\n                    \n#                     #forward pass\n#                     results = model(images)\n#                     losses = criteria(results, targets.unsqueeze(1).float())\n                    \n#                     #no backward pass\n                    \n#                     epoch_val_loss += losses.item()\n                    \n#                     #rounding basically thresholds the output at 0.5\n#                     epoch_val_acc += torch.sum(results.round() == targets.unsqueeze(1).float())\n                    \n#                     val_step +=1\n        \n#                 #after going through the entire dataset calculate the validation loss and accuracy\n#                 epoch_val_loss /= val_step\n#                 val_losses.append(epoch_val_loss)\n#                 epoch_val_acc /= n_val_samples\n#                 val_accs.append(epoch_val_acc)\n        \n#         print(f\"Epoch {epoch} Validation Loss: {epoch_train_loss:.4f}\")\n#         print(f\"Epoch {epoch} Validation Acc: {epoch_train_acc:.4f}\")\n    \n#     return train_losses, val_losses, train_accs, val_accs","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:19:27.896398Z","iopub.status.idle":"2024-08-08T13:19:27.896893Z","shell.execute_reply.started":"2024-08-08T13:19:27.896652Z","shell.execute_reply":"2024-08-08T13:19:27.896671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}